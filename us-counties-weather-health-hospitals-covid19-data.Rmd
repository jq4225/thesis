---
title: "US Counties: Weather, Health, & COVID19 Data"
output:
  html_document:
    toc: true
---

# Introduction

*If you want to get started with using this dataset see:* **[Intro to the US Counties COVID19 Data](https://www.kaggle.com/johnjdavisiv/intro-to-the-us-counties-covid19-data/)**

The 3,142 counties of the United States span a diverse range of social, economic, health, and weather conditions. County-level data on health, socioeconomics, and weather can help us address one of the primary tasks of the UNCOVER challenge, which is to identify which populations are at the greatest risk for COVID19. 

My goal in this notebook is to provide a reproducible workflow to help data scientists and researchers study COVID19 in the United States. If you want to use this data, **here is a direct link to the final .csv output:**  https://www.kaggle.com/johnjdavisiv/us-counties-covid19-weather-sociohealth-data

This notebook combines the following data sources:  

## Data sources

* New York Times county-level COVID19 case and fatality data *(part of UNCOVER data)*
     * Documentation with notes on geographic exceptions (NYC and Kansas City) is at the [NYT Github](https://github.com/nytimes/covid-19-data)
* 2016 CDC Social Vulnerability Data *(part of UNCOVER data)*
* 2020 Community Health Rankings Data *(part of UNCOVER data)*
* [NOAA Global Surface Summary of the Day](https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.ncdc:C00516) (GSOD) weather data for 2020
     * Each county is paired with the nearest weather station. Most stations are within 50 km of the county center, and virtually all are within 100 km of the county center
* [Kaiser Family Foundation Data](https://www.kff.org/coronavirus-policy-watch/stay-at-home-orders-to-fight-covid19/) on state-level stay-at-home orders

Temperature and humidity may affect the transmissibility of COVID19, but in the United States, warmer regions also tend to have markedly different socioeconomic and health demographics. Warmer regions also tended to declare stay-at-home orders later, or not at all. It's important to be able to include variables that measure obesity, diabetes, access to healthcare, poverty rates, and social distancing orders, since these factors likely play a role in COVID19 transmission and fatality rates. 

As with all my COVID-related work, everything here is public domain and can be used for any purpose. PM me on Kaggle or [connect with me on Twitter](https://twitter.com/JDruns) if you use this dataset for an interesting project or if you have any questions. Special shoutout to @davidbnn92, whose excellent [Python notebook on COVID19 weather data](https://www.kaggle.com/davidbnn92/weather-data) inspired me to use the GSOD weather dataset.

Please let me know if you find any problems or have any ideas for improvement!

# Changelog / To-Do  

 * **2020-12-05**: Latest data updated as cases continue to rise in most of the US
 * **2020-11-07**: US now over 100,000 new cases per day. Latest data updated.
 * **2020-10-24**: US nearing a third wave: temps down, cases up. Updated with latest data.
 * **2020-09-17**: Re-run again, still works!!
 * **2020-05-04**: Re-run with latest COVID19 data again  
 * **2020-04-17**: Re-run with latest COVID19 data  
 * **2020-04-13**: Notebook is public! Minor variable name fixes.
 * **2020-04-12**: Added "pseudo-counties" for NYC and KC.
 * **2020-04-11**: CDC and County Health data use an external file for more flexible variable selection  
 * **2020-04-10**: Now using CDC data for county centers. NYC and KC have locations, added SpatialPolygon export  
 * **2020-04-10**: Added 3, 5, 10, and 15-day centered rolling averages of min/mean/max temperature and dewpoint  
 * **2020-04-08**: Added Kaiser Family Foundation data on statewide stay at home orders  
 * **2020-04-07**: Initial upload  

**To-do**

* Figure out what the difference between near-duplicate variables in the county health rankings
     * e.g. `unemployment` and `unemployment_2` 
* Add metafile on column data types and sources  

# Initial setup

```{r setup, message = FALSE}
library(tidyverse)
library(geosphere) #Needed for Haversine distance
library(readxl)
library(rgeos)
library(zoo)
library(sp)

#All file paths
cdc_vuln_file <- "../input/uncover/UNCOVER/esri_covid-19/esri_covid-19/cdcs-social-vulnerability-index-svi-2016-overall-svi-county-level.csv"
gsod_directory_file <- "../input/gsod-file-directory/GSOD_directory.txt"
noaa_station_file <- "../input/noaa-gsod-stations-list/NOAA_GSOD_stations_clean.txt"
county_health_file <- "../input/uncover/UNCOVER/county_health_rankings/county_health_rankings/us-county-health-rankings-2020.csv"
kff_statewide_stay_at_home_file <- "../input/covid19-statewide-stay-at-home-orders/KFF-dot-org-Statewide-Stay-at-Home-Orders.xlsx"
county_health_column_select_file <- "../input/county-health-column-selection/county_health_column_selection.csv"
cdc_column_select_file <- "../input/cdc-social-vulnerability-column-selections/cdc_social_vulnerability_column_select.csv"

# Restrict weather data so we only look at stations with recent data
min_weather_end_date <- as.Date("2020-04-01")

#For NYC munging
nyc_boroughs_fips <- c("36081",
                       "36085",
                       "36061",
                       "36047",
                       "36005")
#For KSC munging
#Want Jackson, Clay, Platte, and Cass counties
ksc_county_fips <- c("29037", "29047", 
                     "29095", "29165")

```

# CDC social vulnerability data

The US Centers for Diease Control and Prevention releases data on many different metrics of "social vulnerability" to natural and man-made disasters. Broadly these can be grouped into four categories of risk factors: socioeconomic, household composition and disability, minority status and language, and housing / transportation. The CDC provides many raw metrics, a margin of error, and percentile ranks on each of these four categories, plus an overall vulnerability percentile, for each county. 

I'm using an external vector to extract variables of interest, and rename them to something intelligible. The GIS data is extracted, saved separately, and used to get lat/long for each county. The [full documentation can be found here](https://svi.cdc.gov/Documents/Data/2016_SVI_Data/SVI2016Documentation.pdf) (thanks to @lzx1126 for finding the documentation).

Throughout the notebook we'll be using the `fips` (Federal Information Processing Standards) codes to join together county-level data. Importantly, we need to zero-pad all FIPS codes so they are all five digit strings. 

Processing the county polygon data with `readWKT()` throws some warnings about negative areas for a few counties, but I'm not really sure why. Leave a comment if you know why this is happening! It doesn't seem to affect the county location that comes out of `coordinates()` but it might cause issues for later GIS use.

## New York City and Kansas City peculiarities

One recurrent theme in this notebook will be the need to deal specially with New York City (NYC) and Kansas City (KC or KSC). Because of how the New York Times is aggregating COVID19 data, these cities are each reported as one unit, even though NYC is made up of five separate counties and KC overlaps four separate counties. Social, economic, and health data from the CDC and Healthy County Rankings data are only available at the county level. As a workaround, I have created "pseudo-counties" for NYC and KC. These have `fips` codes of `"NYC"` and `"KSC"`.

For absolute measures, like population or total number of people who are unemployed, I take the sum of the four/five (NYC/KSC) counties that make up the city. For relative measures, like the prevalence of diabetes, I take a weighted average, using the population of each county as weight. **This is not a perfect fix**, but until we have county-level data for NYC and KC, this is the best fix I could come up with. Leave a comment if you have thoughts or ideas for improvement.    

```{r cdc, warning = FALSE}

#If anyone knows why these negative area warnings happen, please let me know! 
cdc_social_vuln_raw <- read.csv(cdc_vuln_file, stringsAsFactors = FALSE)

cdc_column_selection_df <- read_csv(cdc_column_select_file,
                                    col_names = TRUE,
                                    col_types = "cccc")

#Get geometry for GIS if desired
us_county_geometry <- cdc_social_vuln_raw %>%
  select(state, county, fips, geometry, 
         objectid, shape_are, shape_len) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad="0"))

county_centers <- data.frame(fips = character(nrow(us_county_geometry)), 
                             lat=numeric(nrow(us_county_geometry)), 
                             lon=numeric(nrow(us_county_geometry)),
                             stringsAsFactors = FALSE)

#Use polygons to get (approximate) county center locations
for (i in 1:nrow(us_county_geometry)){
  this_polygon <- readWKT(us_county_geometry$geometry[i])
  county_centers$fips[i] <- us_county_geometry$fips[i]
  
  #Note coordinates() gives (long, lat) for some reason
  county_centers$lat[i] <- coordinates(this_polygon)[2]
  county_centers$lon[i] <- coordinates(this_polygon)[1]
}

#Add NYC and KC
#Data source is Wolfram Alpha
county_centers <- county_centers %>%
  bind_rows(data.frame(fips = "NYC", lat = 40.66, lon = -73.94,
                       stringsAsFactors = FALSE),
            data.frame(fips = "KSC", lat = 39.13, lon = -94.55,
                       stringsAsFactors = FALSE)
            )

#Extract area and social vulnerability data
#Also give more intelligible names
cdc_social_vuln_2016 <- cdc_social_vuln_raw %>%
  select(all_of(cdc_column_selection_df$variable_name)) %>%
  rename_at(vars(cdc_column_selection_df$variable_name), 
            ~cdc_column_selection_df$detailed_name) %>%
  mutate(fips = str_pad(as.character(fips), 5, pad="0")) %>%
  select(fips, county, area_sqmi, total_population, 
         everything())


# Deal with NYC and KSC issue
nyc_county_pops <- cdc_social_vuln_2016 %>%
  filter(fips %in% nyc_boroughs_fips) %>%
  pull(total_population)
nyc_pop <- sum(nyc_county_pops)
nyc_county_weights <- nyc_county_pops / nyc_pop

nyc_cdc <- cdc_social_vuln_2016 %>%
  filter(fips %in% nyc_boroughs_fips)

#set 1:2 for NYC
nyc_cdc[6,"fips"] <- "NYC"
nyc_cdc[6,"county"] <- "New York City"

for (i in 3:ncol(nyc_cdc)){
  if (cdc_column_selection_df$variable_type[i] == "relative" || 
      cdc_column_selection_df$variable_type[i] == "percentile") {
    #For relative measures or percentiles, take weighted average
    nyc_cdc[6,i] <- sum(as.data.frame(nyc_cdc)[1:5,i]*nyc_county_weights)
  } else {
    #For absolute, just sum
    nyc_cdc[6,i] <- sum(as.data.frame(nyc_cdc)[1:5,i])
  }
}

# Kansas city
ksc_county_pops <- cdc_social_vuln_2016 %>%
  filter(fips %in% ksc_county_fips) %>%
  pull(total_population)
ksc_pop <- sum(ksc_county_pops)
ksc_county_weights <- ksc_county_pops / ksc_pop
ksc_cdc <- cdc_social_vuln_2016 %>%
  filter(fips %in% ksc_county_fips)

#set 1:2 for NYC
ksc_cdc[5,"fips"] <- "KSC"
ksc_cdc[5,"county"] <- "Kansas City"

for (i in 3:ncol(ksc_cdc)){
  if (cdc_column_selection_df$variable_type[i] == "relative" || 
      cdc_column_selection_df$variable_type[i] == "percentile") {
    #For relative measures or percentiles, take weighted average
    ksc_cdc[5,i] <- sum(as.data.frame(ksc_cdc)[1:4,i]*ksc_county_weights)
  } else {
    #For absolute, just sum
    ksc_cdc[5,i] <- sum(as.data.frame(ksc_cdc)[1:4,i])
  }
}

#Tack on NYC and KSC
cdc_social_vuln_2016 <- cdc_social_vuln_2016 %>%
  bind_rows(nyc_cdc %>% slice(6), 
            ksc_cdc %>% slice(5))

```

# NOAA GSOD daily weather data

Kaggle offers BigQuery API integration to pull NOAA weather data into Python notebooks, but I haven't figured out how to do that in R yet. Fortunately, NOAA uploads raw CSVs to their website. If we know the station we are looking for, we can download the data directly. 

Here, I use `GSOD_directory.txt` (which is just copied from the web directory HTML page at NOAA) and `NOAA_GSOD_stations_clean.txt`, an additional file that I found [on NOAA's website](https://www.ncdc.noaa.gov/data-access/land-based-station-data/station-metadata), which has a list of all of the weather stations in the GSOD database. Based on some earlier versions of this script, I also exclude a few specific stations because they have a lot of missing data or other issues. 

The station locations  is a fixed-width file, which makes reading it very tedious. Nevertheless, we end up with a great plot of all of the GSOD weather stations in the world! 

```{r noaa_gsod}

#Read directly from NOAA
gsod_url <- "https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/2020"

gsod_filenames <- read.table(gsod_directory_file, header = FALSE,
                             stringsAsFactors = FALSE,
                             col.names = c("file","last_modified","time","size"))

#These stations have lots of missing data or other issues, so ignore them.
bad_stations <- c("72211800482", #Sarasota FL
                  "72334703809", #Dyer, TN
                  "99818099999", #Sanilac, MI
                  "99726099999", #Sanilac MI
                  "72522904858",
                  "72340703953",
                  "72028803711",
                  "74003024103", #Tooele, UT
                  "72575399999", #Tooele, UT
                  "91197721508", #Also in the mountains on Hawaii
                  "99999921514") #On top of a volcano at 11,000' in Hawaii

#Set up filenames for all stations
gsod_filenames <- gsod_filenames %>%
  mutate(station_id = sub(".csv", "", file)) %>%
  select(file, last_modified, station_id)

#Reading this fixed-width file is a mess
noaa_col_names <- c("USAF",
                    "WBAN",
                    "STATION_NAME",
                    "CTRY",
                    "ST",
                    "CALL",
                    "LAT",
                    "LON",
                    "ELEV_M",
                    "BEGIN",
                    "END")
#Get station locations
noaa_stations <- read_fwf(noaa_station_file, 
                              fwf_positions(c(1, 8, 14, 44, 49, 52, 58, 66, 75, 83, 92), 
                                            c(7,13, 43, 46, 51, 56, 65, 74, 82, 91, 99), 
                                            noaa_col_names),
                              skip = 1, col_types = "ccccccccccc")

#Must filter by END > a few days ago
#Also filter by BEGIN < Jan 1
#Finally remove the bad stations

#Join location to file names 
noaa_stations <- noaa_stations %>%
  unite(usaf_wban, USAF, WBAN, sep="") %>%
  mutate(LAT = as.numeric(sub("\\+","", LAT)),
         LON = as.numeric(sub("\\+","", LON)),
         ELEV_M = as.numeric(sub("\\+","", ELEV_M)),
         BEGIN = as.Date(BEGIN, format = "%Y%m%d"),
         END = as.Date(END, format = "%Y%m%d")) %>%
  inner_join(gsod_filenames,
             by = c("usaf_wban" = "station_id")) %>%
  filter(END >= min_weather_end_date) %>%
  filter(BEGIN <= as.Date("2020-01-01")) %>%
  filter(!usaf_wban %in% bad_stations)

#Plot station locations
noaa_stations %>%
  ggplot(aes(x=LON, y=LAT)) + 
  geom_point(alpha=0.1) + 
  coord_equal() + 
  ggtitle("NOAA GSOD Weather Station Locations")

```

# Plotting location of each county

We can see that the GSOD weather stations (red) have pretty good coverage: we should be able to find a station that's close to the center of each county. 

```{r county_loc}

#View counties
county_centers %>%
  ggplot(aes(x=lon, y=lat)) + 
  geom_point(alpha=0.5, size=0.5, color = "black") + 
  geom_point(data = noaa_stations, 
             aes(x=LON,y=LAT), 
             color = "red", alpha = 0.1) + 
  coord_fixed(ratio = 1, xlim = c(-170,-60), ylim = c(25,70)) + #Sorry, Hawaii
  ggtitle("US County locations (black) and GSOD weather stations (red)")

```

# US County Health Rankings data

The County Health Rankings are a fantastic dataset of many different measures of community health. In most cases the variables are extremely well-named, and the breadth and completeness of the data are impressive. 

I'm using an external CSV file to specify the variables I'm pulling in. You can explore `us-county-health-rankings-2020.csv` if you want to use other variables.

I also left out some columns with many missing variables. These tended to be measures of racial equality (e.g. breaking down unemployment by race), and the reason so many are missing is because US County Health Rankings sets that variable as missing if the county has too few members of a given race to make any meaningful conclusions. The same is true for rare events, like infant mortality. Few/no events means the rate is set to NA. Feel free to change this if you want, but make sure you have a strategy for the missing data: it's missing for a reason. 

Though I haven't found a codebook, the County Health Rankings website is quite detailed on the methodology. [Here is the methodology](https://www.countyhealthrankings.org/explore-health-rankings/measures-data-sources/county-health-rankings-model/health-factors/social-and-economic-factors/community-safety/homicides) for homicide rate, for example. 

One important question I have not figured out is the difference between some similarly-named variables, like `population` and `population_2`, and `segregation_index` and `segregation_index_2`. Usually these are strongly correllated, but not identical. If you have any ideas please let me know! Two variables need to be renamed to follow R-allowed naming conventions: `20th_percentile_income` is now `twentieth_percentile_income`, ditto for 80th percentile.

```{r county_health, message=FALSE}

county_health <- read_csv(county_health_file)

health_col_select_df <- read_csv(county_health_column_select_file,
                                        col_types = "cc")

#Leaving out (for now) some columns with lots of missing data, esp regarding racial inequalities
#not because it's not important, but because there is lots of missingness
select_health_cols <- as.character(health_col_select_df$variable_name)

#Zero pad FIPS
county_health <- county_health %>%
  select(all_of(select_health_cols)) %>%
  mutate(fips = as.character(fips)) %>%
  mutate(fips = str_pad(fips, 5, pad="0"))

# NYC Stuff
#Using CDC for population. Will take weighted averages
nyc_health <- county_health %>%
  filter(fips %in% nyc_boroughs_fips)

#set 1:3 for NYC
nyc_health[6,"fips"] <- "NYC"
nyc_health[6,"state"] <- "New York"
nyc_health[6,"county"] <- "New York City"

for (i in 4:ncol(nyc_health)){
  if (health_col_select_df$variable_name[i] == "presence_of_water_violation") {
    nyc_health[6,i] <- as.logical(max(as.data.frame(nyc_health)[1:5,i]))
  } else if (health_col_select_df$variable_type[i] == "relative") {
    #For relative measures, take weighted average
    nyc_health[6,i] <- sum(as.data.frame(nyc_health)[1:5,i]*nyc_county_weights)
  } else {
    #For absolute, just sum
    nyc_health[6,i] <- sum(as.data.frame(nyc_health)[1:5,i])
  }
}

# Kansas city stuff
ksc_health <- county_health %>%
  filter(fips %in% ksc_county_fips)

#set 1:3 for KSC
ksc_health[5,"fips"] <- "KSC"
ksc_health[5,"state"] <- "Missouri"
ksc_health[5,"county"] <- "Kansas City"

for (i in 4:ncol(ksc_health)){
  if (health_col_select_df$variable_name[i] == "presence_of_water_violation") {
    ksc_health[5,i] <- as.logical(max(as.data.frame(ksc_health)[1:4,i]))
  } else if (health_col_select_df$variable_type[i] == "relative") {
    #For relative measures, take weighted average
    ksc_health[5,i] <- sum(as.data.frame(ksc_health)[1:4,i]*ksc_county_weights)
  } else {
    #For absolute, just sum
    ksc_health[5,i] <- sum(as.data.frame(ksc_health)[1:4,i])
  }
}

#Add to county health df, rename two trouble-causing variables
county_health <- bind_rows(county_health, 
                           ksc_health %>% slice(5),
                           nyc_health %>% slice(6)) %>%
  rename(eightieth_percentile_income = `80th_percentile_income`,
         twentieth_percentile_income = `20th_percentile_income`)

```

# Join CDC, health, and location data

This is just a simple join with a tiny bit of manual cleaning. The District of Columbia has two entries, so we delete the one that does not join up with the health data. Population density, surprisingly, isn't in either dataset, so we add it here.

```{r first_join}

county_health_loc <- county_health %>%
  full_join(county_centers, 
            by = "fips") %>%
  full_join(cdc_social_vuln_2016 %>% select(-county), by = "fips",
            suffix = c("_CHR", "_CDC")) %>%
  mutate(population_density_per_sqmi = total_population / area_sqmi) %>%
  select(fips, state, county, lat, lon, 
         total_population,  area_sqmi, 
         population_density_per_sqmi,
         everything()) %>%
  filter(fips != "11000") %>% #DC has a duplicate entry
  filter(!is.na(county)) #Remove statewide data

#save county-level data
write.csv(county_health_loc, 
          "us_county_sociohealth_data.csv", 
          row.names = FALSE)

```

# Pair counties with weather stations 

Here, we use the latitude and longitude of each GSOD weather station, and the latitude and longitude of the center of each county, to find the closest weather station. That's the station we will use to get daily weather data. The distance is computed using the Haversine formula, implemented in the `distHaversine` function from the `geosphere` library. We also save the distance to the closest weather station, in kilometers.

I also add a 3,5,10, and 15-day rolling average for the max temp, mean temp, min temp, and dewpoint for each county. This is a "centered" rolling average. It's nice to compute this now, because we won't have missing data at the beginning of the timeseries for the period we actually care about (i.e. Feb/March and onward). The `Unknown or uninitialised column` warning is, I think, a [known bug](https://stackoverflow.com/questions/39041115/fixing-a-multiple-warning-unknown-column) but let me know if you can fix it.

We can also check to see how close each county is to the nearest weather station. Virtually all are close to or below 100 km (~62 miles). 

```{r weather_find}

#There is probably a clever way to vectorize this..
# but I can't figure it out! 

#Weather station distance matrix
noaa_longlat <- cbind(noaa_stations$LON, noaa_stations$LAT)

#For each county...make distance matrix, find nearest

for (i in 1:nrow(county_health_loc)) {
  #print(i) #for monitoring progress
  
  #For each county...
  this_county_loc <- cbind(county_health_loc[i,"lon"], 
                           county_health_loc[i,"lat"]) 
  
  #Get distances to all stations - in km
  distance_to_stations <- distHaversine(this_county_loc, noaa_longlat)
  
  #Note closest station
  closest_ind <- which.min(distance_to_stations)
  closest_station_id <- noaa_stations[closest_ind,"usaf_wban"]
  
  county_health_loc$closest_station_usaf_wban[i] <- as.character(closest_station_id)
  county_health_loc$km_to_closest_station[i] <- distance_to_stations[closest_ind]/1000
  
} 

#Distribution of distances to closest station
county_health_loc %>%
  ggplot(aes(x=km_to_closest_station)) + 
  geom_histogram(binwidth=5)

# Join county data with closest GSOD station
county_health_loc_stations <- county_health_loc %>% 
  left_join(noaa_stations, 
            by = c("closest_station_usaf_wban" = "usaf_wban")) %>%
  select(-CTRY, -ST, -LAT, -LON, -BEGIN, -END)

```

# Download 2020 daily weather data

Here, we download the daily weather data for 2020 (Jan 1st until today) from 3,142 weather stations. **This takes a fairly long time!** We are pulling directly from a URL, not using an API. 

Since we are not using an API, the data here are in "Freedom Units" (i.e. NOT metric!) so keep that in mind. I also rename the variables to more intelligible variable names and split up the weather indicator (fog, rain, snow, hail, thunder, tornado) into dummy variables. NOAA uses a variety of indicator variables for missing data (99.9, 999.9, and 9999.9 so we have to set those to NA).

```{r weather_download, warning = FALSE}

# Pull CSVs directly from NOAA's server
#  This takes between 12 and 60 minutes

#The ol' loop and bind_rows strategy
all_county_weather <- list()

#Probably a cleverer coder could vectorize or lapply this
for (i in 1:nrow(county_health_loc_stations)){
  #print(i) #Tracks progress
  #For each county, get the daily weather data for 2020
  this_county_fips <- county_health_loc_stations$fips[i]
  
  this_county_weather_file <- county_health_loc_stations$file[i]
  this_county_weather_url <- paste(gsod_url, this_county_weather_file, sep="/")
  
  this_county_weather <- read_csv(this_county_weather_url,
                  col_types = cols(
                    STATION = col_character(),
                    DATE = col_date(format = ""),
                    LATITUDE = col_double(),
                    LONGITUDE = col_double(),
                    ELEVATION = col_double(),
                    NAME = col_character(),
                    TEMP = col_double(),
                    TEMP_ATTRIBUTES = col_double(),
                    DEWP = col_double(),
                    DEWP_ATTRIBUTES = col_double(),
                    SLP = col_double(),
                    SLP_ATTRIBUTES = col_double(),
                    STP = col_double(),
                    STP_ATTRIBUTES = col_double(),
                    VISIB = col_double(),
                    VISIB_ATTRIBUTES = col_double(),
                    WDSP = col_double(),
                    WDSP_ATTRIBUTES = col_double(),
                    MXSPD = col_double(),
                    GUST = col_double(),
                    MAX = col_double(),
                    MAX_ATTRIBUTES = col_character(),
                    MIN = col_double(),
                    MIN_ATTRIBUTES = col_character(),
                    PRCP = col_double(),
                    PRCP_ATTRIBUTES = col_character(),
                    SNDP = col_double(),
                    FRSHTT = col_character()
                    ))
                  
  #Only keeping relevant/useful data
  #Note: These are still in Freedom Units, not metric
  clean_weather_data <- this_county_weather %>%
    transmute(station_id = STATION,
              station_name = NAME,
              station_lat = LATITUDE,
              station_lon = LONGITUDE,
              date = DATE, 
              mean_temp = TEMP,
              min_temp = MIN,
              max_temp = MAX,
              dewpoint = DEWP,
              sea_level_pressure = SLP,
              station_pressure = STP,
              visibility = VISIB,
              wind_speed = WDSP,
              max_wind_speed = MXSPD,
              wind_gust = GUST,
              precipitation = PRCP,
              precip_flag = PRCP_ATTRIBUTES,
              FRSHTT = FRSHTT) %>%
    separate(FRSHTT, into = c("fog", "rain", "snow", 
                      "hail", "thunder", "tornado"),
             sep=c(1,2,3,4,5)) %>%
    mutate(county_fips = this_county_fips)
  
  #999.9 is the missing data indicator, also 9999.9 and 99.9
  clean_weather_data[clean_weather_data == 99.9] <- NA
  clean_weather_data[clean_weather_data == 99.99] <- NA
  clean_weather_data[clean_weather_data == 999.9] <- NA
  clean_weather_data[clean_weather_data == 9999.9] <- NA
  
  #Moving averages
  clean_weather_data_with_avgs <- clean_weather_data %>%
    mutate(mean_temp_3d_avg = rollmean(mean_temp, 3, na.pad = TRUE, align = "center")) %>%
    mutate(mean_temp_5d_avg = rollmean(mean_temp, 5, na.pad = TRUE, align = "center")) %>%
    mutate(mean_temp_10d_avg = rollmean(mean_temp, 10, na.pad = TRUE, align = "center")) %>%
    mutate(mean_temp_15d_avg = rollmean(mean_temp, 15, na.pad = TRUE, align = "center")) %>%
    mutate(max_temp_3d_avg = rollmean(max_temp, 3, na.pad = TRUE, align = "center")) %>%
    mutate(max_temp_5d_avg = rollmean(max_temp, 5, na.pad = TRUE, align = "center")) %>%
    mutate(max_temp_10d_avg = rollmean(max_temp, 10, na.pad = TRUE, align = "center")) %>%
    mutate(max_temp_15d_avg = rollmean(max_temp, 15, na.pad = TRUE, align = "center")) %>% 
    mutate(min_temp_3d_avg = rollmean(min_temp, 3, na.pad = TRUE, align = "center")) %>%
    mutate(min_temp_5d_avg = rollmean(min_temp, 5, na.pad = TRUE, align = "center")) %>%
    mutate(min_temp_10d_avg = rollmean(min_temp, 10, na.pad = TRUE, align = "center")) %>%
    mutate(min_temp_15d_avg = rollmean(min_temp, 15, na.pad = TRUE, align = "center")) %>%
    mutate(dewpoint_3d_avg = rollmean(dewpoint, 3, na.pad = TRUE, align = "center")) %>%
    mutate(dewpoint_5d_avg = rollmean(dewpoint, 5, na.pad = TRUE, align = "center")) %>%
    mutate(dewpoint_10d_avg = rollmean(dewpoint, 10, na.pad = TRUE, align = "center")) %>%
    mutate(dewpoint_15d_avg = rollmean(dewpoint, 15, na.pad = TRUE, align = "center"))
  
  #Store in list
  all_county_weather[[i]] <- clean_weather_data_with_avgs
}

#Put it all together
all_county_weather_df <- bind_rows(all_county_weather)

#Plot
all_county_weather_df %>%
  ggplot(aes(x=date, y=mean_temp_3d_avg, 
             group = factor(station_id))) + 
  geom_line(alpha = 0.04, color = "blue") + 
  ylab("Mean daily temperature (F)") + 
  theme(legend.position = "none")

```

# New York Times COVID19 case and fatality data

The New York Times daily data on COVID19 cases and deaths by county underpins all of this. We need to load the data and make a few changes so it joins well with our health, location, and weather data. Documentation is at the [NYT Github](https://github.com/nytimes/covid-19-data). The dataset includes some other areas that do not have county FIPS codes, like US territories. I'm adding abbreviations for those places for now. I prefer reading the data directly from GitHub since it is updated so frequently. 

```{r nyt}

nyt_url <- "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv"
#can replace with the Kaggle filepath too but this is updated more often

#Drop in cities, territories, etc.
covid_county <- read_csv(nyt_url, col_types = "ccccnn") %>%
  mutate(date = as.Date(date)) %>%
  mutate(fips = as.character(fips)) %>%
  mutate(fips = ifelse(county == "New York City", "NYC", fips)) %>%
  mutate(fips = ifelse(county == "Kansas City", "KSC", fips)) %>%
  mutate(fips = ifelse(state == "Virgin Islands", "VI", fips)) %>%
  mutate(fips = ifelse(state == "Guam", "GU", fips)) %>%
  mutate(fips = ifelse(state == "Puerto Rico", "PR", fips)) %>%
  mutate(fips = ifelse(state == "Northern Mariana Islands", "NMR", fips)) %>%
  filter(county != "Unknown") %>%
  filter(!fips %in% nyc_boroughs_fips) #Avoid double-counting

```

# Statewide stay-at-home orders

Thanks to a dataset from the [Kaiser Family Foundation](https://www.kff.org/coronavirus-policy-watch/stay-at-home-orders-to-fight-covid19/), I can incorporate data on whether a county is under a statewide stay-at-home / "shelter in place" order on any given day. The KFF dataset includes two dates, the day the stay-at-home order was *announced*, and the day it went into effect. Both are included here as binary yes/no variable. 

Note that some states (at least as of April 12th) have not declared a stay-at-home order, and no national stay-at-home order has been declared yet. Also, some individual cities, like Miami and New York City, might have declared stay-at-home orders before their respective states declared a stay-at-home order. If you have county-level data on stay-at-home orders, please let me know! 

```{r stay_at_home}

covid_county_full <- covid_county %>%
  left_join(county_health_loc_stations, by = "fips", 
            suffix = c("_nyt", "_aug")) %>%
  left_join(all_county_weather_df,
            by = c("date" = "date", "fips" = "county_fips")) %>%
  mutate(county = county_nyt, state = state_nyt) %>%
  select(-county_nyt, -state_nyt, -state_aug, 
         -county_aug, -last_modified,
         -STATION_NAME,
         -station_lat, -station_lon, -file,
         -closest_station_usaf_wban) %>%
  select(date, county, state, fips, cases, deaths, 
         everything())

#Import statewide stay at home orders
stay_at_home <- read_excel(kff_statewide_stay_at_home_file,
                           skip=3,col_names = c("state", 
                                                "date_stay_at_home_announced",
                                                "date_stay_at_home_effective")) %>%
  mutate(date_stay_at_home_announced = as.Date(date_stay_at_home_announced,
                                               format = "%b %d")) %>%
  mutate(date_stay_at_home_effective = as.Date(date_stay_at_home_effective,
                                               format = "%b %d")) %>%
  filter(!is.na(date_stay_at_home_announced))

covid_with_stay_home <- covid_county_full %>% 
  left_join(stay_at_home, by = "state") %>%
  mutate(stay_at_home_announced = ifelse(date >= date_stay_at_home_announced,
                                         "yes", "no")) %>%
  mutate(stay_at_home_effective = ifelse(date >= date_stay_at_home_effective,
                                         "yes", "no")) %>%
  mutate(stay_at_home_announced = ifelse(is.na(stay_at_home_announced),
                                         "no", stay_at_home_announced)) %>%
  mutate(stay_at_home_effective = ifelse(is.na(date_stay_at_home_effective),
                                         "no", stay_at_home_effective)) %>%
  select(date, county, state, fips, cases, deaths, 
         stay_at_home_announced, 
         stay_at_home_effective,
         everything()) 

#Export to csv
file_save_name <- "US_counties_COVID19_health_weather_data.csv"
write_csv(covid_with_stay_home, file_save_name)
write_csv(us_county_geometry, "us_county_geometry.csv")

```

# Final joins and CSV export

Now all that's left is one big join. Our final dataset **will not include all counties or all weather data**, because counties only appear in the New York Times dataset once they have at least one case. We'll export in "long" format to facilitate longitudinal data analysis. Each row is one day, in one county. 

If you are building machine learning or statistical models with this data, **I strongly recommend predicting `log(cases)`** as opposed to `cases`. Ditto for fatalities. [See my notebook here](https://www.kaggle.com/johnjdavisiv/gamms-for-covid19-prediction-leak-proof) for why this works so well. 

Careful with missing data. As with any real dataset, we have some missingness. For many features this missingness is **not** completely at random. The gold standard would be to deal with this using multiple imputation, but I'll leave that for another day. 

That's all there is! I hope you enjoyed this data cleaning adventure. Toss me an upvote or leave a comment if you found this notebook useful, and definitely let me know if you find anything interesting using this dataset (or any problems!). Good luck and stay safe out there. 

-J